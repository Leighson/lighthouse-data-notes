{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Topic Modeling Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:12.932082Z",
     "start_time": "2020-04-29T12:18:12.200358Z"
    }
   },
   "outputs": [],
   "source": [
    "# import TfidfVectorizer and CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# import NMF and LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:14.463840Z",
     "start_time": "2020-04-29T12:18:13.020189Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a variable called `'no_features'` and set its value to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:15.590700Z",
     "start_time": "2020-04-29T12:18:15.585820Z"
    }
   },
   "outputs": [],
   "source": [
    "no_features = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a variable `'no_topics'` and set its value to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:16.743304Z",
     "start_time": "2020-04-29T12:18:16.737763Z"
    }
   },
   "outputs": [],
   "source": [
    "no_topics = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate a TfidfVectorizer with the following parameters:\n",
    "\n",
    "\n",
    "    * max_df = 0.95\n",
    "    * min_df = 2\n",
    "    * max_features = no_features\n",
    "    * stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:17.892838Z",
     "start_time": "2020-04-29T12:18:17.888668Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_df = 0.95,\n",
    "    min_df = 2,\n",
    "    max_features = no_features,\n",
    "    stop_words = 'english'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use fit_transform method of TfidfVectorizer to transform the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:21.486038Z",
     "start_time": "2020-04-29T12:18:19.135830Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_docs = tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the features names from TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:22.661253Z",
     "start_time": "2020-04-29T12:18:22.656169Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate NMF and fit transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:24.532755Z",
     "start_time": "2020-04-29T12:18:23.661009Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf = NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0186895 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.04587933, 0.        ,\n",
       "        0.02482667],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.07081967,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01037582, 0.        , 0.        , ..., 0.        , 0.03395286,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.fit_transform(tfidf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA w/ Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate a CountVectorizer with following parameters:\n",
    "\n",
    "\n",
    "    * max_df = 0.95\n",
    "    * min_df = 2\n",
    "    * max_features = no_features\n",
    "    * stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:25.547906Z",
     "start_time": "2020-04-29T12:18:25.540452Z"
    }
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(\n",
    "    max_df = 0.95,\n",
    "    min_df = 2,\n",
    "    max_features = no_features,\n",
    "    stop_words='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use fit_transform method of CountVectorizer to transform documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:29.307223Z",
     "start_time": "2020-04-29T12:18:26.637153Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = count.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the features names from TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:31.516381Z",
     "start_time": "2020-04-29T12:18:31.498740Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_feature_mames = count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sure': 80,\n",
       " 'did': 22,\n",
       " 'world': 97,\n",
       " 'try': 88,\n",
       " 'think': 85,\n",
       " 'government': 38,\n",
       " 'look': 53,\n",
       " 'got': 37,\n",
       " 'power': 64,\n",
       " 'people': 62,\n",
       " 'read': 69,\n",
       " 'need': 59,\n",
       " 'little': 50,\n",
       " 'just': 42,\n",
       " 'new': 60,\n",
       " 'don': 27,\n",
       " 'like': 47,\n",
       " 'know': 44,\n",
       " 'question': 68,\n",
       " 'want': 93,\n",
       " 'work': 96,\n",
       " 'key': 43,\n",
       " 'drive': 28,\n",
       " 'use': 89,\n",
       " 'going': 35,\n",
       " 'probably': 65,\n",
       " 'doesn': 26,\n",
       " 'year': 98,\n",
       " 'line': 48,\n",
       " 'time': 86,\n",
       " 'right': 71,\n",
       " 'does': 25,\n",
       " 'll': 51,\n",
       " 'good': 36,\n",
       " 'let': 46,\n",
       " 'mail': 55,\n",
       " 'edu': 29,\n",
       " 'jesus': 41,\n",
       " 'day': 21,\n",
       " 'lot': 54,\n",
       " '20': 6,\n",
       " '25': 7,\n",
       " 'better': 14,\n",
       " 'file': 32,\n",
       " 'mr': 58,\n",
       " 'say': 74,\n",
       " 'make': 56,\n",
       " 'way': 94,\n",
       " '10': 1,\n",
       " 'years': 99,\n",
       " 'used': 90,\n",
       " 'tell': 81,\n",
       " 'thanks': 82,\n",
       " 'list': 49,\n",
       " 'available': 9,\n",
       " 'help': 39,\n",
       " 'information': 40,\n",
       " 'software': 77,\n",
       " 'data': 20,\n",
       " '12': 2,\n",
       " '14': 3,\n",
       " '15': 4,\n",
       " 'space': 78,\n",
       " '16': 5,\n",
       " 'com': 17,\n",
       " 'number': 61,\n",
       " 'things': 84,\n",
       " 'run': 72,\n",
       " 'program': 67,\n",
       " 'set': 76,\n",
       " 'windows': 95,\n",
       " 'bit': 15,\n",
       " 'best': 13,\n",
       " 'state': 79,\n",
       " 'course': 19,\n",
       " 'different': 24,\n",
       " 'come': 18,\n",
       " 'using': 91,\n",
       " 'point': 63,\n",
       " 'long': 52,\n",
       " '00': 0,\n",
       " 'really': 70,\n",
       " 'god': 34,\n",
       " 'case': 16,\n",
       " 'said': 73,\n",
       " 'believe': 12,\n",
       " 'fact': 30,\n",
       " 'thing': 83,\n",
       " 'far': 31,\n",
       " 'second': 75,\n",
       " 'law': 45,\n",
       " 'problem': 66,\n",
       " 'didn': 23,\n",
       " 'true': 87,\n",
       " 've': 92,\n",
       " 'max': 57,\n",
       " 'ax': 10,\n",
       " 'g9v': 33,\n",
       " 'a86': 8,\n",
       " 'b8f': 11}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate LatentDirichletAllocation and fit transformed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:03.315322Z",
     "start_time": "2020-04-29T12:18:32.768365Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_docs = lda.fit_transform(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01      , 0.01000171, 0.01000244, ..., 0.01000133, 0.0100006 ,\n",
       "        0.01000094],\n",
       "       [0.01111111, 0.01111215, 0.01111294, ..., 0.01111269, 0.01111411,\n",
       "        0.01111166],\n",
       "       [0.01111111, 0.01111119, 0.01111122, ..., 0.01111227, 0.0111123 ,\n",
       "        0.01111157],\n",
       "       ...,\n",
       "       [0.03333333, 0.03333348, 0.03333405, ..., 0.0333348 , 0.03333477,\n",
       "        0.0333343 ],\n",
       "       [0.02      , 0.02000013, 0.02000119, ..., 0.81998184, 0.02000439,\n",
       "        0.02000021],\n",
       "       [0.004     , 0.00400093, 0.00400079, ..., 0.00400101, 0.00400078,\n",
       "        0.00400013]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function `display_topics` that is able to display the top words in a topic for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:04.476192Z",
     "start_time": "2020-04-29T12:19:04.469045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "did just ll data years going don drive edu fact\n",
      "Topic 1:\n",
      "thanks 14 file data years going don drive edu fact\n",
      "Topic 2:\n",
      "does know just ll data years god don drive edu\n",
      "Topic 3:\n",
      "edu 14 file just ll data line going don drive\n",
      "Topic 4:\n",
      "just don like tell ll ve years god doesn drive\n",
      "Topic 5:\n",
      "like 14 just file data years going don drive edu\n",
      "Topic 6:\n",
      "just a86 g9v does think doesn don drive edu fact\n",
      "Topic 7:\n",
      "use don ve data years going doesn drive edu fact\n",
      "Topic 8:\n",
      "people don just mr ll ve years going drive edu\n",
      "Topic 9:\n",
      "good 14 ll just data years going don drive edu\n",
      "Topic 10:\n",
      "think don years going doesn drive edu fact far file\n",
      "Topic 11:\n",
      "god don tell jesus 14 just ll 25 years going\n",
      "Topic 12:\n",
      "time just ll ve did want key doesn don drive\n",
      "Topic 13:\n",
      "windows tell data years going doesn don drive edu fact\n",
      "Topic 14:\n",
      "drive problem power file data years does don edu fact\n",
      "Topic 15:\n",
      "tell ll power mail program software data doesn did years\n",
      "Topic 16:\n",
      "don ll data want did years going drive edu fact\n",
      "Topic 17:\n",
      "ve b8f did good don drive edu fact far file\n",
      "Topic 18:\n",
      "bit ax just 10 let file power years going drive\n",
      "Topic 19:\n",
      "com file just 14 data set mail line want going\n",
      "Topic 20:\n",
      "need file ll just data mail want going don drive\n",
      "Topic 21:\n",
      "used ax just ve want did going doesn don drive\n",
      "Topic 22:\n",
      "year 14 10 ll just better ve did want far\n",
      "Topic 23:\n",
      "right 10 25 did good don drive edu fact far\n",
      "Topic 24:\n",
      "problem file ve want really did god doesn don drive\n",
      "Topic 25:\n",
      "make 14 just ll data mail did years going drive\n",
      "Topic 26:\n",
      "mail ax mr file ll let good don drive edu\n",
      "Topic 27:\n",
      "way just ll data did years going don drive edu\n",
      "Topic 28:\n",
      "want ll data set line did going don drive edu\n",
      "Topic 29:\n",
      "10 b8f a86 20 did years got edu fact far\n",
      "Topic 30:\n",
      "want new ll just work ve program mail really years\n",
      "Topic 31:\n",
      "really ll data did years going drive edu fact far\n",
      "Topic 32:\n",
      "said mr file just ll let data doesn did key\n",
      "Topic 33:\n",
      "say just ll mr data doesn did key jesus don\n",
      "Topic 34:\n",
      "space 14 file just 10 ll power 25 data want\n",
      "Topic 35:\n",
      "point 14 10 ll data line doesn did going edu\n",
      "Topic 36:\n",
      "work 14 file just ll doesn want did going don\n",
      "Topic 37:\n",
      "file a86 g9v data set line mail program going drive\n",
      "Topic 38:\n",
      "believe file ve doesn did don drive edu fact far\n",
      "Topic 39:\n",
      "got just file ll ve did going don drive edu\n",
      "Topic 40:\n",
      "using 14 file just ve ll really going doesn don\n",
      "Topic 41:\n",
      "sure file ll new data doesn did god don drive\n",
      "Topic 42:\n",
      "question ax mr ll doesn did years going drive edu\n",
      "Topic 43:\n",
      "probably ll new data doesn going don drive edu fact\n",
      "Topic 44:\n",
      "ll b8f g9v doesn did good don drive edu fact\n",
      "Topic 45:\n",
      "key file new information 10 going using data program doesn\n",
      "Topic 46:\n",
      "going ax ll better just ve doesn did years drive\n",
      "Topic 47:\n",
      "years 14 10 ll better just let ve new doesn\n",
      "Topic 48:\n",
      "program 14 file just let better ll help 10 run\n",
      "Topic 49:\n",
      "help file thanks data set mail doesn really g9v don\n",
      "Topic 50:\n",
      "true file just let set doesn line did god don\n",
      "Topic 51:\n",
      "power 14 10 let better just ve ll doesn did\n",
      "Topic 52:\n",
      "better a86 g9v ve doesn did going don drive edu\n",
      "Topic 53:\n",
      "case ax power better let ve doesn did years drive\n",
      "Topic 54:\n",
      "software 14 know file new use ll better data line\n",
      "Topic 55:\n",
      "information 14 file 10 just thanks 25 edu data mail\n",
      "Topic 56:\n",
      "best better data doesn did years drive edu fact far\n",
      "Topic 57:\n",
      "number 14 file let better 10 25 doesn fact good\n",
      "Topic 58:\n",
      "read ax mr file ll just let ve doesn did\n",
      "Topic 59:\n",
      "doesn ll better let data going don drive edu fact\n",
      "Topic 60:\n",
      "didn tell let ll file better just line doesn did\n",
      "Topic 61:\n",
      "look file let ll better 25 did years good drive\n",
      "Topic 62:\n",
      "list 14 file 10 let just better ll thanks new\n",
      "Topic 63:\n",
      "data ax 14 10 file just available let better ve\n",
      "Topic 64:\n",
      "law 14 government file let information just new data doesn\n",
      "Topic 65:\n",
      "let a86 g9v know government ll doesn did god don\n",
      "Topic 66:\n",
      "world just let government new better ve ll doesn did\n",
      "Topic 67:\n",
      "government people make mr new let like state ll years\n",
      "Topic 68:\n",
      "lot people ll let better ve did really don drive\n",
      "Topic 69:\n",
      "00 14 20 15 did got drive edu fact far\n",
      "Topic 70:\n",
      "long 14 just new better ll ve really drive going\n",
      "Topic 71:\n",
      "try file problem just let ll better ve doesn information\n",
      "Topic 72:\n",
      "course new let data years going don drive edu fact\n",
      "Topic 73:\n",
      "things like let better new thing going ll ve data\n",
      "Topic 74:\n",
      "available a86 g9v know thanks government data mail line really\n",
      "Topic 75:\n",
      "set ax 14 file just let better ve use don\n",
      "Topic 76:\n",
      "day 14 just let 10 new jesus ll doesn did\n",
      "Topic 77:\n",
      "thing like tell better let little doesn did 25 got\n",
      "Topic 78:\n",
      "line ax 14 file 10 just let better ll ve\n",
      "Topic 79:\n",
      "run problem does just file let ll better 10 thanks\n",
      "Topic 80:\n",
      "little ll come ve let didn thanks max going data\n",
      "Topic 81:\n",
      "state 14 people fact mr file 10 ll come new\n",
      "Topic 82:\n",
      "second 14 people 10 let better ll come just going\n",
      "Topic 83:\n",
      "fact people 14 tell mr like let come better new\n",
      "Topic 84:\n",
      "different ax problem tell come like better things thanks going\n",
      "Topic 85:\n",
      "come 14 just let jesus ll better going things new\n",
      "Topic 86:\n",
      "far know people 14 let better just jesus going things\n",
      "Topic 87:\n",
      "jesus time people know does make point 14 god believe\n",
      "Topic 88:\n",
      "12 14 10 just mr 25 better ll let new\n",
      "Topic 89:\n",
      "mr b8f g9v people know time tell make believe things\n",
      "Topic 90:\n",
      "20 14 10 believe years god better come 25 said\n",
      "Topic 91:\n",
      "15 14 10 just file ll mr 25 let better\n",
      "Topic 92:\n",
      "16 14 ax 10 mr ll just 25 point file\n",
      "Topic 93:\n",
      "25 b8f 14 god 20 going come drive years new\n",
      "Topic 94:\n",
      "14 a86 b8f g9v people 12 tell say like things\n",
      "Topic 95:\n",
      "max ax 14 mr know ll problem 10 tell didn\n",
      "Topic 96:\n",
      "ax b8f a86 g9v max 25 mr 20 long lot\n",
      "Topic 97:\n",
      "new b8f time people know does make tell problem point\n",
      "Topic 98:\n",
      "know time people does tell make believe point say thanks\n",
      "Topic 99:\n",
      "new g9v try look question list drive mail got key\n",
      "Topic 0:\n",
      "ax max g9v b8f a86 14 mr 25 ll 12\n",
      "Topic 1:\n",
      "10 drive 20 12 16 15 25 14 year power\n",
      "Topic 2:\n",
      "00 list mail new day got ve 15 20 thanks\n",
      "Topic 3:\n",
      "god good jesus believe does time think true people say\n",
      "Topic 4:\n",
      "file windows thanks program does know use help set like\n",
      "Topic 5:\n",
      "like just don people want make think really know way\n",
      "Topic 6:\n",
      "people said don did know think say right mr going\n",
      "Topic 7:\n",
      "problem use bit using used law time try work run\n",
      "Topic 8:\n",
      "key data information use used new number software available law\n",
      "Topic 9:\n",
      "edu com space available mail information program list set new\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f'Topic {topic_idx}:')\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* display top 10 words from each topic from NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:05.672461Z",
     "start_time": "2020-04-29T12:19:05.656545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "did just ll data years going don drive edu fact\n",
      "Topic 1:\n",
      "thanks 14 file data years going don drive edu fact\n",
      "Topic 2:\n",
      "does know just ll data years god don drive edu\n",
      "Topic 3:\n",
      "edu 14 file just ll data line going don drive\n",
      "Topic 4:\n",
      "just don like tell ll ve years god doesn drive\n",
      "Topic 5:\n",
      "like 14 just file data years going don drive edu\n",
      "Topic 6:\n",
      "just a86 g9v does think doesn don drive edu fact\n",
      "Topic 7:\n",
      "use don ve data years going doesn drive edu fact\n",
      "Topic 8:\n",
      "people don just mr ll ve years going drive edu\n",
      "Topic 9:\n",
      "good 14 ll just data years going don drive edu\n",
      "Topic 10:\n",
      "think don years going doesn drive edu fact far file\n",
      "Topic 11:\n",
      "god don tell jesus 14 just ll 25 years going\n",
      "Topic 12:\n",
      "time just ll ve did want key doesn don drive\n",
      "Topic 13:\n",
      "windows tell data years going doesn don drive edu fact\n",
      "Topic 14:\n",
      "drive problem power file data years does don edu fact\n",
      "Topic 15:\n",
      "tell ll power mail program software data doesn did years\n",
      "Topic 16:\n",
      "don ll data want did years going drive edu fact\n",
      "Topic 17:\n",
      "ve b8f did good don drive edu fact far file\n",
      "Topic 18:\n",
      "bit ax just 10 let file power years going drive\n",
      "Topic 19:\n",
      "com file just 14 data set mail line want going\n",
      "Topic 20:\n",
      "need file ll just data mail want going don drive\n",
      "Topic 21:\n",
      "used ax just ve want did going doesn don drive\n",
      "Topic 22:\n",
      "year 14 10 ll just better ve did want far\n",
      "Topic 23:\n",
      "right 10 25 did good don drive edu fact far\n",
      "Topic 24:\n",
      "problem file ve want really did god doesn don drive\n",
      "Topic 25:\n",
      "make 14 just ll data mail did years going drive\n",
      "Topic 26:\n",
      "mail ax mr file ll let good don drive edu\n",
      "Topic 27:\n",
      "way just ll data did years going don drive edu\n",
      "Topic 28:\n",
      "want ll data set line did going don drive edu\n",
      "Topic 29:\n",
      "10 b8f a86 20 did years got edu fact far\n",
      "Topic 30:\n",
      "want new ll just work ve program mail really years\n",
      "Topic 31:\n",
      "really ll data did years going drive edu fact far\n",
      "Topic 32:\n",
      "said mr file just ll let data doesn did key\n",
      "Topic 33:\n",
      "say just ll mr data doesn did key jesus don\n",
      "Topic 34:\n",
      "space 14 file just 10 ll power 25 data want\n",
      "Topic 35:\n",
      "point 14 10 ll data line doesn did going edu\n",
      "Topic 36:\n",
      "work 14 file just ll doesn want did going don\n",
      "Topic 37:\n",
      "file a86 g9v data set line mail program going drive\n",
      "Topic 38:\n",
      "believe file ve doesn did don drive edu fact far\n",
      "Topic 39:\n",
      "got just file ll ve did going don drive edu\n",
      "Topic 40:\n",
      "using 14 file just ve ll really going doesn don\n",
      "Topic 41:\n",
      "sure file ll new data doesn did god don drive\n",
      "Topic 42:\n",
      "question ax mr ll doesn did years going drive edu\n",
      "Topic 43:\n",
      "probably ll new data doesn going don drive edu fact\n",
      "Topic 44:\n",
      "ll b8f g9v doesn did good don drive edu fact\n",
      "Topic 45:\n",
      "key file new information 10 going using data program doesn\n",
      "Topic 46:\n",
      "going ax ll better just ve doesn did years drive\n",
      "Topic 47:\n",
      "years 14 10 ll better just let ve new doesn\n",
      "Topic 48:\n",
      "program 14 file just let better ll help 10 run\n",
      "Topic 49:\n",
      "help file thanks data set mail doesn really g9v don\n",
      "Topic 50:\n",
      "true file just let set doesn line did god don\n",
      "Topic 51:\n",
      "power 14 10 let better just ve ll doesn did\n",
      "Topic 52:\n",
      "better a86 g9v ve doesn did going don drive edu\n",
      "Topic 53:\n",
      "case ax power better let ve doesn did years drive\n",
      "Topic 54:\n",
      "software 14 know file new use ll better data line\n",
      "Topic 55:\n",
      "information 14 file 10 just thanks 25 edu data mail\n",
      "Topic 56:\n",
      "best better data doesn did years drive edu fact far\n",
      "Topic 57:\n",
      "number 14 file let better 10 25 doesn fact good\n",
      "Topic 58:\n",
      "read ax mr file ll just let ve doesn did\n",
      "Topic 59:\n",
      "doesn ll better let data going don drive edu fact\n",
      "Topic 60:\n",
      "didn tell let ll file better just line doesn did\n",
      "Topic 61:\n",
      "look file let ll better 25 did years good drive\n",
      "Topic 62:\n",
      "list 14 file 10 let just better ll thanks new\n",
      "Topic 63:\n",
      "data ax 14 10 file just available let better ve\n",
      "Topic 64:\n",
      "law 14 government file let information just new data doesn\n",
      "Topic 65:\n",
      "let a86 g9v know government ll doesn did god don\n",
      "Topic 66:\n",
      "world just let government new better ve ll doesn did\n",
      "Topic 67:\n",
      "government people make mr new let like state ll years\n",
      "Topic 68:\n",
      "lot people ll let better ve did really don drive\n",
      "Topic 69:\n",
      "00 14 20 15 did got drive edu fact far\n",
      "Topic 70:\n",
      "long 14 just new better ll ve really drive going\n",
      "Topic 71:\n",
      "try file problem just let ll better ve doesn information\n",
      "Topic 72:\n",
      "course new let data years going don drive edu fact\n",
      "Topic 73:\n",
      "things like let better new thing going ll ve data\n",
      "Topic 74:\n",
      "available a86 g9v know thanks government data mail line really\n",
      "Topic 75:\n",
      "set ax 14 file just let better ve use don\n",
      "Topic 76:\n",
      "day 14 just let 10 new jesus ll doesn did\n",
      "Topic 77:\n",
      "thing like tell better let little doesn did 25 got\n",
      "Topic 78:\n",
      "line ax 14 file 10 just let better ll ve\n",
      "Topic 79:\n",
      "run problem does just file let ll better 10 thanks\n",
      "Topic 80:\n",
      "little ll come ve let didn thanks max going data\n",
      "Topic 81:\n",
      "state 14 people fact mr file 10 ll come new\n",
      "Topic 82:\n",
      "second 14 people 10 let better ll come just going\n",
      "Topic 83:\n",
      "fact people 14 tell mr like let come better new\n",
      "Topic 84:\n",
      "different ax problem tell come like better things thanks going\n",
      "Topic 85:\n",
      "come 14 just let jesus ll better going things new\n",
      "Topic 86:\n",
      "far know people 14 let better just jesus going things\n",
      "Topic 87:\n",
      "jesus time people know does make point 14 god believe\n",
      "Topic 88:\n",
      "12 14 10 just mr 25 better ll let new\n",
      "Topic 89:\n",
      "mr b8f g9v people know time tell make believe things\n",
      "Topic 90:\n",
      "20 14 10 believe years god better come 25 said\n",
      "Topic 91:\n",
      "15 14 10 just file ll mr 25 let better\n",
      "Topic 92:\n",
      "16 14 ax 10 mr ll just 25 point file\n",
      "Topic 93:\n",
      "25 b8f 14 god 20 going come drive years new\n",
      "Topic 94:\n",
      "14 a86 b8f g9v people 12 tell say like things\n",
      "Topic 95:\n",
      "max ax 14 mr know ll problem 10 tell didn\n",
      "Topic 96:\n",
      "ax b8f a86 g9v max 25 mr 20 long lot\n",
      "Topic 97:\n",
      "new b8f time people know does make tell problem point\n",
      "Topic 98:\n",
      "know time people does tell make believe point say thanks\n",
      "Topic 99:\n",
      "new g9v try look question list drive mail got key\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* display top 10 words from each topic from LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:06.842806Z",
     "start_time": "2020-04-29T12:19:06.831721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "ax max g9v b8f a86 14 mr 25 ll 12\n",
      "Topic 1:\n",
      "10 drive 20 12 16 15 25 14 year power\n",
      "Topic 2:\n",
      "00 list mail new day got ve 15 20 thanks\n",
      "Topic 3:\n",
      "god good jesus believe does time think true people say\n",
      "Topic 4:\n",
      "file windows thanks program does know use help set like\n",
      "Topic 5:\n",
      "like just don people want make think really know way\n",
      "Topic 6:\n",
      "people said don did know think say right mr going\n",
      "Topic 7:\n",
      "problem use bit using used law time try work run\n",
      "Topic 8:\n",
      "key data information use used new number software available law\n",
      "Topic 9:\n",
      "edu com space available mail information program list set new\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch: Use LDA w/ Gensim to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lighthouse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f6bdb6bebcaa6c165db367a86e382ae8df75f09257c40532b854029e0e3d706f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
