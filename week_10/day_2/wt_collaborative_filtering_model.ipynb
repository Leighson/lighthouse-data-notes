{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems and Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data input for a recommender system can be thought of as a large matrix with the rows indicating an entry for a customer, and the columns indicating an entry for a particular item, $R$. Then entry $R_{ij}$ will contain the score for that customer $_i$ has given to product $_j$. The downside is that user's tend not to provide enough reviews for the matrix.\n",
    "\n",
    "Recommender systems aim to fill in this missing informationby predicting the customer score of items where the score is missing. Then recommender systems will recommend items to the customer that have the highest score.\n",
    "\n",
    "![recommender_matrix](https://miro.medium.com/max/1400/1*iwQf4YzX_iIBf1dYfkrblw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works by trying to factorise the matrix $R$ into two lower dimensional matrices $U$ and $V$, such that $R = U^{T}V$. Suppose that $R$ has dimension $d_1 \\times d_2$, then $U$ will have dimensions $D \\times d_1$ and $V$ will have dimensions $D \\times d_2$. Here, $D$ is chosen by the user and needs to be large enough to encode the nuances of $R$, but not too large so that it affects performance and/or lead to overfitting. A typical size od $D$ is 20.\n",
    "\n",
    "While this appears simple, it is complicated by the missing data. Imputing the data might work but it makes the methods very slow. Instead, popular methods focus only on the matrix entries $R_{ij}$ that are known and fit the factorisation to minimise the error of these known $R_{ij}$. This still suffers the issue that it might overfit, however, but in those cases, _regularisation_ methods may be used to reduce that issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import io\n",
    "import zipfile\n",
    "import surprise\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ''\n",
    "with open('./res/data/filmtrust/ratings.txt', 'r') as file:\n",
    "    df = pd.read_table(file, sep=' ', names=['uid', 'iid', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>iid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid  iid  rating\n",
       "0    1    1     2.0\n",
       "1    1    2     4.0\n",
       "2    1    3     3.5\n",
       "3    1    4     3.0\n",
       "4    1    5     4.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data in a `sparse` format. In a `sparse` format, the first column is the row number of the matrix $ii$; the second column is the column number of the matrix $jj$; and the third column is the matrix entry $R_{ij}R_{ij}$.\n",
    "\n",
    "For this dataset, the first column is the user ID, the second is the movie ID reviewed, and the third column is the review score. This `sparse` format is also the input that matrix factorisation methods require, rather than the full matrix $RR$, because they only use the non-missing matrix entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into `surprise` using the `Reader` class. The main thing the `Reader` class does is to specify the range of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review range 0.5 to 4.0.\n"
     ]
    }
   ],
   "source": [
    "# check the range of the reviews for this dataset\n",
    "lower_rating = df['rating'].min()\n",
    "upper_rating = df['rating'].max()\n",
    "\n",
    "print(f'Review range {lower_rating} to {upper_rating}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale = (0.5, 4.))\n",
    "data = surprise.Dataset.load_from_df(df, reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SVD++` algorithm extends `SVD` by only optimizing known terms and performing regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = surprise.SVDpp()\n",
    "output = svd.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE_**: It's not good to train the model on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0028030537791928\n"
     ]
    }
   ],
   "source": [
    "# the uids and iids should be set as strings\n",
    "pred = svd.predict(uid='50', iid='52')\n",
    "score = pred.est\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score of 3 but in order to recommend the best products to users, we need to find $n$ items that have the highest predicted score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foucsing on `uid` = 50 and find one item to recommend them. First, find the movie ids that user 50 didn't rate. We don't want to recommend them a movie they're already watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all movie ids\n",
    "iids = df['iid'].unique()\n",
    "\n",
    "# get list of iids that uid 50 has rated\n",
    "iids50 = df.loc[df['uid']==50, 'iid']\n",
    "\n",
    "# remove the iids that uid 50 has rated from the list of all movie ids\n",
    "iids_to_pred = np.setdiff1d(iids, iids50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, predict the score of each of the modie ids that user 50 didn't rate and find the best score. Create another dataset with the `iids` we want to predict in the sparse format as before: `uid`, `iid`, `rating`.\n",
    "\n",
    "Arbitrarily set all the ratings of this test set to 4, as they are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=50, iid=14, r_ui=4.0, est=3.0888411005388203, details={'was_impossible': False})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = [[50, iid, 4.] for iid in iids_to_pred]\n",
    "predictions = svd.test(testset)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top item for user 50 has iid 126 with predicted rating 4.0.\n"
     ]
    }
   ],
   "source": [
    "pred_ratings = np.array([pred.est for pred in predictions])\n",
    "\n",
    "# find the index of the maximum predicted rating\n",
    "i_max = pred_ratings.argmax()\n",
    "\n",
    "# find the corresponding lid to recommend\n",
    "iid = iids_to_pred[i_max]\n",
    "\n",
    "print(f'Top item for user 50 has iid {iid} with predicted rating {pred_ratings[i_max]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing your own recommender system, you will normally have metadata which allows you to retrieve specific information that this dataset is lacking.\n",
    "\n",
    "Similarly, it is possible to output the top $n$ items for user 50 by replacing the `argmax()` method with the `argparition()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is bad practice to fit a model on the dataset without checking its performance and tuning parameters which affect the fit.\n",
    "\n",
    "Like other matrix factorisation algorithms, the method `SVD++` will depend on a number of main tuning constants:\n",
    "- the dimension $DD$ affecting the size of $UU$ and $VV$\n",
    "- the learning rate\n",
    "- the regularisation term\n",
    "- the number of epochs\n",
    "\n",
    "In `surprise`, tuning is performed using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr_all': 0.01, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'lr_all' : [0.001, 0.01],\n",
    "    'reg_all' : [0.1, 0.5],\n",
    "}\n",
    "\n",
    "gs = surprise.model_selection.GridSearchCV(\n",
    "    surprise.SVDpp, param_grid=param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "# print combination of parameters that gave best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8279  0.8217  0.8332  0.8254  0.8332  0.8283  0.0045  \n",
      "MAE (testset)     0.6547  0.6518  0.6565  0.6537  0.6612  0.6556  0.0032  \n",
      "Fit time          7.99    7.78    7.95    7.95    7.89    7.91    0.07    \n",
      "Test time         0.15    0.14    0.13    0.14    0.14    0.14    0.01    \n"
     ]
    }
   ],
   "source": [
    "alg = surprise.SVDpp(lr_all = 0.001) # according to grid search results\n",
    "output = surprise.model_selection.cross_validate(alg, data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lighthouse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6bdb6bebcaa6c165db367a86e382ae8df75f09257c40532b854029e0e3d706f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
