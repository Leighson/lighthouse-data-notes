{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we will be working with this [dataset](https://drive.google.com/file/d/1B07fvYosBNdIwlZxSmxDfeAf9KaygX89/view?usp=sharing), where we will be predicting sales. \n",
    "\n",
    "**The main goal is to create a `pipeline` that covers all the data preprocessing and modeling steps.**\n",
    "\n",
    "\n",
    "**TASK 1**: Build a pipeline that ends with a regression model, to predict `Item_Outlet_Sales` from the dataset. \n",
    "\n",
    "**The pipeline should have following steps:**\n",
    "\n",
    "1. Split the features into numerical and categorical (text)\n",
    "2. Replace null values\n",
    "    - the mean for numerical variables\n",
    "    - the most frequent value for categorical variables\n",
    "3. Create dummy variables from categorical features\n",
    "4. Use a PCA to reduce number of dummy variables to 3 principal components. PCA will be used directly after the OneHotEncoder that outputs data into a SparseMatrix, so we will need to use the **ToDenseTransformer** from the [article about custom pipelines](https://queirozf.com/entries/scikit-learn-pipelines-custom-pipelines-and-pandas-integration).\n",
    "5. Select the 3 best candidates from the original numerical features using KBest\n",
    "6. Fit a Ridge regression (default alpha is fine for now)\n",
    "\n",
    "**TASK 2**: Tune the parameters of multiple models as well as the preprocessing steps and find the best solution.\n",
    "- Try these models: \n",
    "        - Random Forest Regressor\n",
    "        - Gradient Boosting Regressor \n",
    "        - Ridge Regression. \n",
    "- For the task 2, we will need to use the same approach from this [earlier article](https://iaml.it/blog/optimizing-sklearn-pipelines), in the section `PIPELINE TUNING (ADVANCED VERSION)`, where we tried different kinds of scalers. (Use the article as reference.)\n",
    "\n",
    "_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat         0.016047   \n",
       "1           DRC01         5.92          Regular         0.019278   \n",
       "2           FDN15        17.50          Low Fat         0.016760   \n",
       "3           FDX07        19.20          Regular         0.000000   \n",
       "4           NCD19         8.93          Low Fat         0.000000   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat  141.6180            OUT049   \n",
       "3  Fruits and Vegetables  182.0950            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.1380  \n",
       "1  Supermarket Type2           443.4228  \n",
       "2  Supermarket Type1          2097.2700  \n",
       "3      Grocery Store           732.3800  \n",
       "4  Supermarket Type1           994.7052  "
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "data_path = Path('./data')\n",
    "df = pd.read_csv(data_path / \"regression_exercise.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# separate target variable from the original dataframe\n",
    "y = df[\"Item_Outlet_Sales\"]\n",
    "x = df.drop([\"Item_Outlet_Sales\",\"Item_Identifier\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into a train and test set.\n",
    "\n",
    "**Note:** We should always do this at the beginning before the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Task I\n",
    "\n",
    "### Split Features into numerical and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = x.dtypes[x.dtypes == 'object'].index.tolist()\n",
    "num_feats = x.dtypes[~(x.dtypes == 'object')].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Using own function in Pipeline\n",
    "def numFeat(data):\n",
    "    return data[num_feats]\n",
    "\n",
    "def catFeat(data):\n",
    "    return data[cat_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start two separate pipelines for each type of features\n",
    "keep_num = FunctionTransformer(numFeat)\n",
    "keep_cat = FunctionTransformer(catFeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replacing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer numerical\n",
    "impute_num = SimpleImputer(strategy='mean')\n",
    "# df_num_imputed = impute_num.fit_transform(numFeat(xtrain))\n",
    "\n",
    "# impute categorical\n",
    "impute_cat = SimpleImputer(strategy='most_frequent')\n",
    "# df_cat_imputed = impute_cat.fit_transform(catFeat(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# df_cat_endcoded = encoder.fit_transform(df_cat_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA to reduce the number of dummy variables to 3 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget ToDenseTransformer after one hot encoder\n",
    "class ToDenseTransformer():\n",
    "    def transform(self, x, y=None, **fit_params):\n",
    "        return x.todense()\n",
    "    \n",
    "    def fit(self, x, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "dense = ToDenseTransformer()\n",
    "# dense.fit(df_cat_endcoded)\n",
    "# df_cat_dense = dense.transform(df_cat_endcoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def npconvert(data):\n",
    "    return np.asarray(data)\n",
    "\n",
    "to_array = FunctionTransformer(npconvert)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "# df_cat_pca = pca.fit_transform(np.asarray(df_cat_dense))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the 3 best numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "kbest = SelectKBest(k=3)\n",
    "# df_num_kbest = kbest.fit_transform(df_num_imputed, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Use base_model in Task I\n",
    "ridge_model = Ridge()\n",
    "forest_model = RandomForestRegressor()\n",
    "gradient_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# build the features around the pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    ('cat_select', keep_cat),\n",
    "    ('impute_cat', impute_cat),\n",
    "    ('encoder', encoder),\n",
    "    ('dense', dense),\n",
    "    ('convert', to_array),\n",
    "    ('pca', pca)\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('num_select', keep_num),\n",
    "    ('impute_num', impute_num),\n",
    "    ('kbest', kbest)\n",
    "])\n",
    "\n",
    "features = FeatureUnion([\n",
    "    ('features_num', num_pipe),\n",
    "    ('features_cat', cat_pipe)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('features', features),\n",
    "    ('model', ridge_model)\n",
    "])\n",
    "\n",
    "# random forest\n",
    "forest_pipeline = Pipeline([\n",
    "    ('features', features),\n",
    "    ('model', forest_model)\n",
    "])\n",
    "\n",
    "# random forest\n",
    "gradient_pipeline = Pipeline([\n",
    "    ('features', features),\n",
    "    ('model', gradient_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "ridge_pipeline.fit(xtrain, ytrain)\n",
    "ypred_ridge = ridge_pipeline.predict(xtest)\n",
    "\n",
    "# forest regression\n",
    "forest_pipeline.fit(xtrain, ytrain)\n",
    "ypred_forest = forest_pipeline.predict(xtest)\n",
    "\n",
    "# gradient regression\n",
    "gradient_pipeline.fit(xtrain, ytrain)\n",
    "ypred_gradient = gradient_pipeline.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4081355031074001\n",
      "0.5628468906722521\n",
      "0.6073410281290602\n"
     ]
    }
   ],
   "source": [
    "# model.score(df_test,y_test)\n",
    "print(ridge_pipeline.score(xtest, ytest))\n",
    "print(forest_pipeline.score(xtest, ytest))\n",
    "print(gradient_pipeline.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Task II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid parameters\n",
    "ridge_params = {\n",
    "    'features__features_cat__pca__n_components' : [5],\n",
    "    'features__features_num__kbest__k' : [3],\n",
    "    'features__features_num__impute_num__strategy' : ['mean', 'medium'],\n",
    "    'features__features_cat__encoder__drop' : ['first', None],\n",
    "    'model__alpha' : [0.3],\n",
    "}\n",
    "\n",
    "forest_params = {\n",
    "    'features__features_cat__pca__n_components' : [5],\n",
    "    'features__features_num__kbest__k' : [2],\n",
    "    'features__features_num__impute_num__strategy' : ['mean', 'medium'],\n",
    "    'features__features_cat__encoder__drop' : ['if_binary', None],\n",
    "    'model__n_estimators' : [50],\n",
    "    'model__max_depth' : [5],\n",
    "    'model__min_samples_split' : [3]\n",
    "}\n",
    "\n",
    "gradient_params = {\n",
    "    'features__features_cat__pca__n_components' : [2, 3, 4, 5],\n",
    "    'features__features_num__kbest__k' : [2, 3, 4, 5],\n",
    "    'features__features_num__impute_num__strategy' : ['mean', 'medium'],\n",
    "    'features__features_cat__encoder__drop' : ['first', 'if_binary'],\n",
    "    'model__loss' : ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "    'model__learning_rate' : [0.01, 0.03, 0.1, 0.3, 1],\n",
    "    'model__n_estimators' : [10, 50, 100, 110],\n",
    "    'model__max_depth' : [2, 3, 4, 5],\n",
    "    'model__min_samples_split' : [2, 3, 4, 5],\n",
    "    'model__criterion' : ['squared_error', 'mse', 'mae']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1172, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1194, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 426, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/base.py\", line 855, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/impute/_base.py\", line 319, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/impute/_base.py\", line 244, in _validate_input\n",
      "    raise ValueError(\n",
      "ValueError: Can only use these strategies: ['mean', 'median', 'most_frequent', 'constant']  got strategy=medium\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.43363685        nan 0.38274582        nan]\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.43626077        nan 0.38575781        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(transformer_list=[('features_num',\n",
       "                                                                        Pipeline(steps=[('num_select',\n",
       "                                                                                         FunctionTransformer(func=<function numFeat at 0x17fd1f670>)),\n",
       "                                                                                        ('impute_num',\n",
       "                                                                                         SimpleImputer()),\n",
       "                                                                                        ('kbest',\n",
       "                                                                                         SelectKBest(k=3))])),\n",
       "                                                                       ('features_cat',\n",
       "                                                                        Pipeline(steps=[('cat_select',\n",
       "                                                                                         FunctionTransformer(func=<function catFeat at 0x17fd1f8...\n",
       "                                                                                         FunctionTransformer(func=<function npconvert at 0x17fd1f310>)),\n",
       "                                                                                        ('pca',\n",
       "                                                                                         PCA(n_components=3))]))])),\n",
       "                                       ('model', Ridge())]),\n",
       "             param_grid={'features__features_cat__encoder__drop': ['first',\n",
       "                                                                   None],\n",
       "                         'features__features_cat__pca__n_components': [5],\n",
       "                         'features__features_num__impute_num__strategy': ['mean',\n",
       "                                                                          'medium'],\n",
       "                         'features__features_num__kbest__k': [3],\n",
       "                         'model__alpha': [0.3]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train ridge model and output results\n",
    "tuned_ridge = GridSearchCV(ridge_pipeline, ridge_params, refit=True, cv=5, return_train_score=True)\n",
    "tuned_ridge.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('features_num',\n",
       "                                                 Pipeline(steps=[('num_select',\n",
       "                                                                  FunctionTransformer(func=<function numFeat at 0x17fd1f670>)),\n",
       "                                                                 ('impute_num',\n",
       "                                                                  SimpleImputer()),\n",
       "                                                                 ('kbest',\n",
       "                                                                  SelectKBest(k=3))])),\n",
       "                                                ('features_cat',\n",
       "                                                 Pipeline(steps=[('cat_select',\n",
       "                                                                  FunctionTransformer(func=<function catFeat at 0x17fd1f8b0>)),\n",
       "                                                                 ('impute_cat',\n",
       "                                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                                 ('encoder',\n",
       "                                                                  OneHotEncoder(drop='first',\n",
       "                                                                                handle_unknown='ignore')),\n",
       "                                                                 ('dense',\n",
       "                                                                  <__main__.ToDenseTransformer object at 0x2a6ccb670>),\n",
       "                                                                 ('convert',\n",
       "                                                                  FunctionTransformer(func=<function npconvert at 0x17fd1f310>)),\n",
       "                                                                 ('pca',\n",
       "                                                                  PCA(n_components=5))]))])),\n",
       "                ('model', Ridge(alpha=0.3))])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ridge.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1172, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 1194, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 426, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/base.py\", line 855, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/impute/_base.py\", line 319, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/impute/_base.py\", line 244, in _validate_input\n",
      "    raise ValueError(\n",
      "ValueError: Can only use these strategies: ['mean', 'median', 'most_frequent', 'constant']  got strategy=medium\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.57941771        nan 0.57974215        nan]\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/lighthouse/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.60385591        nan 0.60344616        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(transformer_list=[('features_num',\n",
       "                                                                        Pipeline(steps=[('num_select',\n",
       "                                                                                         FunctionTransformer(func=<function numFeat at 0x17fd1f670>)),\n",
       "                                                                                        ('impute_num',\n",
       "                                                                                         SimpleImputer()),\n",
       "                                                                                        ('kbest',\n",
       "                                                                                         SelectKBest(k=3))])),\n",
       "                                                                       ('features_cat',\n",
       "                                                                        Pipeline(steps=[('cat_select',\n",
       "                                                                                         FunctionTransformer(func=<function catFeat at 0x17fd1f8...\n",
       "                                       ('model', RandomForestRegressor())]),\n",
       "             param_grid={'features__features_cat__encoder__drop': ['if_binary',\n",
       "                                                                   None],\n",
       "                         'features__features_cat__pca__n_components': [5],\n",
       "                         'features__features_num__impute_num__strategy': ['mean',\n",
       "                                                                          'medium'],\n",
       "                         'features__features_num__kbest__k': [2],\n",
       "                         'model__max_depth': [5],\n",
       "                         'model__min_samples_split': [3],\n",
       "                         'model__n_estimators': [50]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train forest model and output results\n",
    "tuned_forest = GridSearchCV(forest_pipeline, forest_params, refit=True, cv=5, return_train_score=True)\n",
    "tuned_forest.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('features_num',\n",
       "                                                 Pipeline(steps=[('num_select',\n",
       "                                                                  FunctionTransformer(func=<function numFeat at 0x17fd1f670>)),\n",
       "                                                                 ('impute_num',\n",
       "                                                                  SimpleImputer()),\n",
       "                                                                 ('kbest',\n",
       "                                                                  SelectKBest(k=2))])),\n",
       "                                                ('features_cat',\n",
       "                                                 Pipeline(steps=[('cat_select',\n",
       "                                                                  FunctionTransformer(func=<function catFeat at 0x17fd1f8b0>)),\n",
       "                                                                 ('impute_cat',\n",
       "                                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                                 ('encoder',\n",
       "                                                                  OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                 ('dense',\n",
       "                                                                  <__main__.ToDenseTransformer object at 0x17fd42340>),\n",
       "                                                                 ('convert',\n",
       "                                                                  FunctionTransformer(func=<function npconvert at 0x17fd1f310>)),\n",
       "                                                                 ('pca',\n",
       "                                                                  PCA(n_components=5))]))])),\n",
       "                ('model',\n",
       "                 RandomForestRegressor(max_depth=5, min_samples_split=3,\n",
       "                                       n_estimators=50))])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_forest.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gradient model and output results\n",
    "tuned_gradient = GridSearchCV(gradient_pipeline, gradient_params, refit=True, cv=5, return_train_score=True)\n",
    "tuned_gradient.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predictions\n",
    "ypred = tuned_ridge.predict(xtest)\n",
    "yprob = tuned_ridge.predict_proba(xtest)\n",
    "\n",
    "# scores\n",
    "print('Final score is: ', tuned_ridge.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predictions\n",
    "ypred = tuned_forest.predict(xtest)\n",
    "yprob = tuned_forest.predict_proba(xtest)\n",
    "\n",
    "# scores\n",
    "print('Final score is: ', tuned_forest.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predictions\n",
    "ypred = tuned_gradient.predict(ytest)\n",
    "yprob = tuned_gradient.predict_proba(ytest)\n",
    "\n",
    "# scores\n",
    "print('Final score is: ', tuned_gradient.score(xtest, ytest))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "f6bdb6bebcaa6c165db367a86e382ae8df75f09257c40532b854029e0e3d706f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lighthouse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
